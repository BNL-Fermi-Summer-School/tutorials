{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b88ebb5",
   "metadata": {},
   "source": [
    "\n",
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b09d9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc703d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboardx scipy matplotlib torchinfo torch-summary tensorflow torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd1281",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "# Eyes and Algorithms\n",
    "\n",
    "`Visual interpretation is essential to living but had remained elusive to machines.`\n",
    "\n",
    "Our ability to interpret images is not just a cognitive skill; it's core to\n",
    "survival. It allows us to navigate our world, recognize emotions, and\n",
    "communicate across cultures. From detecting danger to understanding complex\n",
    "social cues, visual interpretation is fundamental to human existence.\n",
    "\n",
    "In recent years, computers have achieved remarkable success in various domains\n",
    "by applying mathematical methods developed over centuries. However, tasks\n",
    "involving images present unique challenges. Unlike problems with well-defined\n",
    "mathematical descriptions, complex shapes and visual patterns have eluded\n",
    "precise mathematical characterization.\n",
    "\n",
    "But with the advent of neural networks, a new door has opened. Convolutional\n",
    "Neural Networks (CNNs), a special type of neural network, have enabled\n",
    "computers to process images in ways previously thought impossible. By capturing\n",
    "the intricate patterns and structures within images, CNNs have revolutionized\n",
    "the field.\n",
    "\n",
    "The ability to automate image interpretation has far-reaching impacts across various sectors:\n",
    "\n",
    "- **Healthcare**: Enhancing diagnostics (e.g., detecting tumors in X-rays) -\n",
    "  impacting millions of patients.\n",
    "- **Transportation**: Guiding autonomous vehicles - thousands self-driving cars on\n",
    "  the road.\n",
    "- **Security**: Powering facial recognition systems - airport security, organizing\n",
    "  photo albums..\n",
    "- **Personalization**: secure access, image search\n",
    "- Environmental/Climate Monitoring: Analyzing satellite images for climate\n",
    "  studies - tracking changes in over thousands of square kilometers\n",
    "- **Agriculture**: Precision farming through satellite imagery - optimizing crop\n",
    "  yields\n",
    "- **Disaster** Response: Analyzing aerial images for disaster relief planning\n",
    "- **Manufacturing**: Quality control through image inspection in manufacturing\n",
    "  plants.\n",
    "- **Archaeology**: Discovering historical sites through satellite imagery leading\n",
    "  to the discovery of over 200 unknown sites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7307cb07",
   "metadata": {},
   "source": [
    "\n",
    "# What computers sees..\n",
    "\n",
    "`Human: Look at all the shapes, colors, sounds, and patterns! Computer: So... is it 1, or is it 0?`\n",
    "\n",
    "For a computer, an image is an array of numbers. It's how computers \"read\"\n",
    "pictures.\n",
    "\n",
    "- **Pixels**: An image is made of tiny parts called pixels. A pixel is a small\n",
    "  dot in an image.\n",
    "- **Black & White Images**: In black & white images, pixels are either black\n",
    "  (0) or white (1).\n",
    "- **Color Images (RGB)**: In color images, pixels have three numbers for red,\n",
    "  green, and blue.\n",
    "- **RGB Channels**: The three parts of a color image are red, green, and blue.\n",
    "  By changing these, you can make many colors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c31d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================j\n",
    "#                               PLOT IMAGE MATRICES\n",
    "# =============================================================================j\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_matrix(\n",
    "    matrix, title, cmap=None, text_color=\"red\", text_size=12, figsize=(2, 2)\n",
    "    ,show_text=True\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(matrix, cmap=cmap)\n",
    "    if show_text:\n",
    "        for i in range(matrix.shape[0]):\n",
    "            for j in range(matrix.shape[1]):\n",
    "                ax.text(\n",
    "                    j,\n",
    "                    i,\n",
    "                    str(matrix[i, j]),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=text_color,\n",
    "                    fontsize=text_size,\n",
    "                )\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage for a black and white image\n",
    "cross = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 1]])\n",
    "plot_matrix(cross, \"Cross\", cmap=\"gray\")\n",
    "\n",
    "# Example usage for an RGB image\n",
    "red_channel = np.array([[255, 0, 0], [0, 128, 0], [128, 0, 255]])\n",
    "green_channel = np.array([[0, 255, 0], [255, 0, 255], [0, 255, 0]])\n",
    "blue_channel = np.array([[0, 0, 255], [0, 128, 0], [255, 0, 128]])\n",
    "\n",
    "color_image = np.stack((red_channel, green_channel, blue_channel), axis=-1)\n",
    "\n",
    "# Plot the individual color channels\n",
    "plot_matrix(red_channel, \"Red Channel\", cmap=\"Reds\")\n",
    "plot_matrix(green_channel, \"Green Channel\", cmap=\"Greens\")\n",
    "plot_matrix(blue_channel, \"Blue Channel\", cmap=\"Blues\")\n",
    "# Plot the full color image\n",
    "plot_matrix(color_image, \"Color Image\", text_size=8, text_color=\"black\", figsize=(3, 3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff1168",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Image processing\n",
    "\n",
    "Take an `image` with `resolution` of `3x3`. How do we `blur` this image.\n",
    "\n",
    "$$\n",
    "\\text{Image} = \\begin{bmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Kernel\n",
    "\n",
    "Since, number in the pixel reprsents the intensity of the color, `blurring` operation would simply mean `averaging` the intensity of the pixel with its neighbors. To represent this operation with matrix, we can use a `kernel` or `filter` matrix: a `3x3` matrix with all values equal to `1` since we are averaging the intensity with 8 neighbors and itself. (Note: you can normalize the kernel by dividing it with the number of elements in the kernel, in this case 9)\n",
    "\n",
    "$$\n",
    "\\text{Kernel/Filter} = \\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Padding\n",
    "\n",
    "But the pixes in the corner do not have neighbors on all sides. So, we can\n",
    "`pad` the image with zeros to make it a `5x5` matrix. The padded image can be\n",
    "represented as a 5x5 matrix with zero padding:\n",
    "\n",
    "$$\n",
    "\\text{Padded Image} = \\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Convolution Operation:\n",
    "\n",
    "Now we replace each pixel in the image with the average of its neighbors and\n",
    "itself by multiplying the `kernel` with the padded image at each position and\n",
    "summing the result. We `move/stride` the kernel over the image and perform the same\n",
    "operation at each position.\n",
    "\n",
    "This operation is important in many tasks in image processing and beyond. So,\n",
    "it has a special name: `convolution` and can be represented as following in context of our example:\n",
    "\n",
    "$$\n",
    "\\text{Blurred Image}[i, j] = \\sum_{m=-1}^{1} \\sum_{n=-1}^{1} \\text{Padded Image}[i+m, j+n] \\times \\text{Kernel}[m, n]\n",
    "$$\n",
    "\n",
    "where $\\text{Blurred Image}[i, j]$ represents the value of the pixel at position \\((i, j)\\) in the resulting blurred image.\n",
    "\n",
    "After performing the convolution operation, we get the following resulting blurred image:\n",
    "\n",
    "$$\n",
    "\\text{Blurred Image} = \\begin{bmatrix}\n",
    "2 & 3 & 2 \\\\\n",
    "3 & 4 & 3 \\\\\n",
    "2 & 3 & 2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### TLDR: Convolution Operation\n",
    "\n",
    "<!-- ![cnn_animation](imgs/cnn_animation.gif) -->\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/cnn_animation.gif\" />\n",
    "</p>\n",
    "\n",
    "[source](https://en.wikipedia.org/wiki/File:2D_Convolution_Animation.gif)\n",
    "\n",
    "## More Kernel Examples\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Edge Detection} & = \\begin{bmatrix} -1 & -1 & -1 \\\\ -1 & 8 & -1 \\\\ -1 & -1 & -1 \\end{bmatrix}, &\n",
    "\\text{Horizontal Edge} & = \\begin{bmatrix} 1 & 2 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1 \\end{bmatrix}, &\n",
    "\\text{Vertical Edge} & = \\begin{bmatrix} 1 & 0 & -1 \\\\ 2 & 0 & -2 \\\\ 1 & 0 & -1 \\end{bmatrix}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051f56a2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================================================================j\n",
    "#                               VISUALIZE FILTERS\n",
    "# =============================================================================j\n",
    "\n",
    "def convolution(image, kernel):\n",
    "    pad_size = kernel.shape[0] // 2\n",
    "    padded_image = np.pad(image, pad_size, mode=\"constant\", constant_values=0)\n",
    "    result_image = np.zeros_like(image)\n",
    "    for i in range(pad_size, padded_image.shape[0] - pad_size):\n",
    "        for j in range(pad_size, padded_image.shape[1] - pad_size):\n",
    "            result_image[i - pad_size, j - pad_size] = np.sum(\n",
    "                padded_image[\n",
    "                    i - pad_size : i + pad_size + 1, j - pad_size : j + pad_size + 1\n",
    "                ]\n",
    "                * kernel\n",
    "            )\n",
    "    return result_image\n",
    "\n",
    "\n",
    "def visualize_convolution(image, kernel,show_text=True):\n",
    "    # Original Image\n",
    "    plot_matrix(image, \"Original Image\", cmap=\"gray\", text_size=16,show_text=show_text)\n",
    "\n",
    "    # Kernel in Matrix Form\n",
    "    # plot_matrix(kernel, \"Kernel in Matrix Form\", cmap=\"gray\", text_size=16)\n",
    "\n",
    "    # Blurred Image (Result of Convolution)\n",
    "    post_conv_image = convolution(image, kernel)\n",
    "    plot_matrix(\n",
    "        post_conv_image,\n",
    "        \"Image result of Convolution\",\n",
    "        cmap=\"gray\",\n",
    "        text_size=16,\n",
    "        show_text=show_text,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bfa34a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "# original_image = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\n",
    "blur_kernel = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n",
    "original_image = np.random.randint(0, 255, size=(10, 10))\n",
    "visualize_convolution(original_image, blur_kernel,show_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b43b056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================j\n",
    "#                              VISUALIZE COMPLEX FILTERS\n",
    "# =============================================================================j\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_filters():\n",
    "    return {\n",
    "        \"Blur\": np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]) / 9,\n",
    "        \"Edge Detection\": np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]),\n",
    "        \"Horizontal Edge\": np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]]),\n",
    "        \"Vertical Edge\": np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]]),\n",
    "    }\n",
    "\n",
    "\n",
    "class DatasetLoader:\n",
    "    def __init__(self, dataset_name=\"MNIST\", transform=None):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset = self.load_dataset(transform)\n",
    "\n",
    "    def load_dataset(self, transform):\n",
    "        if self.dataset_name == \"MNIST\":\n",
    "            return torchvision.datasets.MNIST(\n",
    "                root=\"./data\", train=True, download=True, transform=transform\n",
    "            )\n",
    "        # You can add other datasets here, e.g.\n",
    "        elif self.dataset_name == \"CIFAR10\":\n",
    "            return torchvision.datasets.CIFAR10(\n",
    "                root=\"./data\", train=True, download=True, transform=transform\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dataset: {self.dataset_name}\")\n",
    "\n",
    "    def get_sample_image(self, label=None, num_samples=1, seed=None, grayscale=False):\n",
    "        if label is None:  # If no specific label is provided, choose one randomly\n",
    "            if seed is not None:\n",
    "                np.random.seed(seed)\n",
    "            label = np.random.choice(self.get_unique_labels())\n",
    "\n",
    "        images = [image for image, label_ in self.dataset if label_ == label]\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        np.random.shuffle(images)\n",
    "        images = images[:num_samples]\n",
    "        if grayscale:\n",
    "            to_grayscale_transform = transforms.Compose(\n",
    "                [transforms.ToPILImage(), transforms.Grayscale(), transforms.ToTensor()]\n",
    "            )\n",
    "            images = [to_grayscale_transform(image) for image in images]\n",
    "        return images\n",
    "\n",
    "    def get_unique_labels(self):\n",
    "        labels = [label for _, label in self.dataset]\n",
    "        return np.unique(labels)\n",
    "\n",
    "    def get_dataloader(self, batch_size=64, shuffle=True, fraction=1.0):\n",
    "        if fraction < 0.0 or fraction > 1.0:\n",
    "            raise ValueError(\"Fraction must be between 0.0 and 1.0.\")\n",
    "\n",
    "        dataset_size = len(self.dataset)\n",
    "        subset_size = int(fraction * dataset_size)\n",
    "\n",
    "        indices = torch.randperm(dataset_size)[:subset_size]\n",
    "\n",
    "\n",
    "        subset = torch.utils.data.dataset.Subset(self.dataset, indices)\n",
    "        return torch.utils.data.DataLoader(\n",
    "            subset, batch_size=batch_size, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "\n",
    "def apply_filters(image, filters):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, len(filters) + 1, 1)\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "    for idx, (name, kernel) in enumerate(filters.items()):\n",
    "        filtered_image = convolution(image, kernel)\n",
    "        plt.subplot(1, len(filters) + 1, idx + 2)\n",
    "        plt.imshow(filtered_image, cmap=\"gray\")\n",
    "        plt.title(name)\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def demo_filter(dataset_name, count=2):\n",
    "    for _ in range(count):\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "        dataset_loader = DatasetLoader(dataset_name=dataset_name, transform=transform)\n",
    "        sample_image_tensor = dataset_loader.get_sample_image(\n",
    "            num_samples=1, grayscale=True\n",
    "        )[0]\n",
    "        sample_image = sample_image_tensor.squeeze().numpy()\n",
    "        apply_filters(sample_image, get_filters())\n",
    "\n",
    "\n",
    "demo_filter(\"MNIST\")\n",
    "demo_filter(\"CIFAR10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2726f0a",
   "metadata": {},
   "source": [
    "# Complex Problems, Simple Filters\n",
    "\n",
    "`Real world problems are too complicated for simple filters to be effective:`\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/obj_detection.jpg\" alt=\"obj_detection\" width=\"300\"/>\n",
    "  <img src=\"imgs/handwriting_meme.jpg\" alt=\"handwriting_meme\" width=\"300\"/>\n",
    "  <!-- <img src=\"imgs/brain_scan.jpg\" alt=\"brain_scan\" width=\"200\"/> -->\n",
    "</p>\n",
    "\n",
    "source[1](https://www.echelon.health/wp-content/uploads/2020/05/brain-scan.jpg)\n",
    "[2](https://www.meme-arsenal.com/memes/8cd3a0c0fbca1d452927c33cd2638030.jpg)\n",
    "[3](https://en.wikipedia.org/wiki/Object_detection#/media/File:Detected-with-YOLO--Schreibtisch-mit-Objekten.jpg)\n",
    "\n",
    "- **Object Recognition**: Filters can't identify specific objects like cars or\n",
    "  dogs, especially in varying shapes and orientations.\n",
    "- **Face Recognition**: They can't recognize individual faces with different\n",
    "  expressions or angles.\n",
    "- **Semantic Segmentation**: Filters can't divide an image into meaningful parts or\n",
    "  understand relationships between pixels.\n",
    "- **Handling Occlusions**: They struggle with partially hidden or overlapped\n",
    "  objects.\n",
    "- **Multiclass Classification**: Filters can't identify multiple classes within an\n",
    "  image.\n",
    "- **Anomaly Detection**: They can't detect subtle abnormalities, e.g., in medical\n",
    "  imaging.\n",
    "- **Texture Recognition**: Recognizing complex textures or patterns is challenging\n",
    "  for simple filters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb092f0",
   "metadata": {},
   "source": [
    "\n",
    "# Need for Neural\n",
    "\n",
    "`NNs are learning-driven, flexible, able to recognize complex patterns, and capable of generalizing across various tasks.`\n",
    "\n",
    "Traditional methods use fixed filters for tasks like edge detection, requiring\n",
    "expert knowledge and often being limited in flexibility. Neural networks\n",
    "surpass traditional methods in several ways:\n",
    "\n",
    "- **Adaptability**: They learn and adapt from data, accommodating a variety of\n",
    "  tasks.\n",
    "- **Hierarchical Learning**: They can recognize simple patterns and combine\n",
    "  them into more complex structures, enabling the capture of intricate\n",
    "  relationships in the data.\n",
    "- **Automatic Feature Learning**: They automatically find important features,\n",
    "  removing the need for manual engineering.\n",
    "- **Robustness**: Neural networks can handle variations in input that might\n",
    "  challenge rigid traditional filters.\n",
    "- **End-to-End Training**: They provide a direct mapping from raw input to\n",
    "  desired output, simplifying the process.\n",
    "- **Generalization**: With proper training, they can effectively work on new,\n",
    "  unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ab8519",
   "metadata": {},
   "source": [
    "\n",
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a795e",
   "metadata": {},
   "source": [
    "## Fully Connected and Bloated\n",
    "\n",
    "`Neural networks considers everything, and that's the problem...`\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"imgs/nn.png\" alt=\"nn\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "Assuming a `12 MP` resolution colored image with dimensions `4000x3000` pixels\n",
    "and three color channels (Red, Green, Blue), the total number of features for\n",
    "each image would be `4000x3000x3`. If this image is fed into a fully connected\n",
    "layer with 1000 hidden units (neurons), the number of weights (parameters) just\n",
    "for this single layer would be:\n",
    "\n",
    "$$\n",
    "\\textrm{FC size = } 4000 \\times 3000 \\times 3 \\times 1000 = 36 \\text{ billion!!}\n",
    "$$\n",
    "\n",
    "for just one layer! If the neural network consists of several fully connected layers, the number of parameters can grow even more significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e88db22",
   "metadata": {},
   "source": [
    "## Fully Connected and Blind\n",
    "\n",
    "`Human: Take this 2D image data. FC: Oh a single list of numbers!`\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{What was shown} &=\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "&\n",
    "\\text{What is seen} &= [1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Fully connected layers are somewhat \"blind\" to certain inherent properties of\n",
    "images that are essential for effective image analysis. Here's what they tend\n",
    "to overlook:\n",
    "\n",
    "- **Pixel Correlation**: Fully connected layers in traditional DNNs ignore the\n",
    "  spatial relationships between neighboring pixels, treating each pixel\n",
    "  independently. ( The patterns of colors formed by putting pixels on opposite\n",
    "  corners of the images may not be useful to determing what is the objecct in\n",
    "  the image.)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"imgs/teddy_img.png\" alt=\"teddy_img\" width=\"300\"/>\n",
    "</p>\n",
    "\n",
    "[source](https://www.youtube.com/watch?v=HGwBXDKFk9I)\n",
    "\n",
    "<!-- TODO: add iamges for each invariances -->\n",
    "\n",
    "- **Translation Invariance**: Unlike CNNs, fully connected layers can't recognize\n",
    "  the same object in different positions, lacking translation invariance.\n",
    "\n",
    "- **Scale and Rotational Sensitivity**: Fully connected layers are sensitive to\n",
    "  variations in the scale and rotation of objects, whereas CNNs can be more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e33ad52",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "# Convolutional Neural Networks\n",
    "\n",
    "` Bring back the filters!`\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"imgs/cnn_layout.png\" alt=\"cnn_layout\" style=\"width:80%;\" />\n",
    "</p>\n",
    "\n",
    "## CNN: learns filters\n",
    "\n",
    "`Dont worry about which filter to use, let the network learn it.`\n",
    "\n",
    "In traditional image processing, filters are manually designed based on expert\n",
    "knowledge for specific tasks, such as edge detection or noise reduction. These\n",
    "filters remain fixed and perform the same operation independent of problem at hand.\n",
    "\n",
    "On the other hand, Convolutional Neural Networks (CNNs) automatically learning\n",
    "filters from the data during the training process. Rather than relying on\n",
    "pre-defined filters, CNNs initialize their filters randomly and iteratively\n",
    "update them based on the training data . The learning process is guided by\n",
    "optimization algorithms like gradient descent, which adjust the filter weights\n",
    "to minimize the difference between the network's predictions and the actual\n",
    "targets. The adaptive nature of CNNs allows them to learn filters that are\n",
    "specifically tailored to the unique characteristics of the data and the\n",
    "specific task at hand.\n",
    "\n",
    "## CNN: filters and features\n",
    "\n",
    "`Filter can go way beyond edge detection.`\n",
    "\n",
    "In the context of CNNs, a feature refers to a specific visual pattern or\n",
    "characteristic of an image that the network has learned to recognize. Features\n",
    "can range from simple attributes like edges or colors to more complex and\n",
    "abstract representations of objects or shapes. At any stage of more than\n",
    "multiple filters can be applied to capture sufficient information required for\n",
    "inference in deeper layers.\n",
    "\n",
    "For example, the CNN in picture above has `four filters in first layer`. When\n",
    "these filters slide over the input image, each one generates a corresponding\n",
    "feature map. These feature maps, when combined, create a new representation of\n",
    "the of the data coming from the previous layer.\n",
    "\n",
    "## CNN: depth\n",
    "\n",
    "`Increase depth to learn complex features with hierarchies.`\n",
    "\n",
    "Images contains hierarchy of features, with more complex features composed of\n",
    "simpler ones. eg: Learning to detect face requires learning to detect eyes,\n",
    "nose, mouth, etc and those features needs to be combined to detect a face. The\n",
    "learning of face can again be used to learn to detect a person and so on..\n",
    "\n",
    "When add more convolution layers, the network learns to recognize more complex\n",
    "patterns by combining the features learned in the previous layers. The first\n",
    "convolutional layers learn simple features like edges and colors, while deeper\n",
    "layers combine them to learn more abstract and complex concepts like facial\n",
    "features or objects.\n",
    "\n",
    "## CNN: Pooling\n",
    "\n",
    "`What is in the image has little to do with how big the image is.`\n",
    "\n",
    "Its often that we do not care how big the image is. Pooling, is a technique\n",
    "used in CNNs to reduce the spatial dimensions (width and height) of the feature\n",
    "maps generated by the convolutional layers. The pooling operation aggregates\n",
    "information from neighboring pixels and outputs a single value, effectively\n",
    "downsampling the feature map. By reducing the spatial resolution, the network\n",
    "becomes less sensitive to small spatial translations, making it more robust to\n",
    "slight changes in the position of features in the image.\n",
    "\n",
    "Pooling can be done using different techniques, such as `max pooling` (selecting\n",
    "the maximum value from a small neighborhood) or `average pooling` (taking the\n",
    "average value).\n",
    "\n",
    "## CNN: Final layers\n",
    "\n",
    "`Reason using features and not pixels`\n",
    "\n",
    "After several convolutional and pooling layers, the high-level reasoning in the\n",
    "neural network is done via fully connected layers. Neurons in a fully connected\n",
    "layer have full connections to all activations in the previous layer, as seen\n",
    "in regular neural networks, allowing the CNN to decide how to combine the\n",
    "features to make the final decision (e.g., classifying the image).\n",
    "\n",
    "## CNN: What did we get out of it?\n",
    "\n",
    "Convolutional Neural Networks (CNNs) were designed with a particular\n",
    "understanding of the properties of images that traditional Deep Neural Networks\n",
    "(DNNs) often ignored. Here are the key ideas behind CNNs that make them so\n",
    "effective for image analysis:\n",
    "\n",
    "- **Local Connectivity**: Recognizing that pixels in images are related to\n",
    "  their immediate neighbors, CNNs employ local receptive fields. Each neuron is\n",
    "  only connected to a small region of the input, which enables the network to\n",
    "  focus on local features. (Kernels to the rescue!)\n",
    "\n",
    "- **Parameter Sharing**: Unlike traditional DNNs, where each weight is unique\n",
    "  to an input, CNNs use parameter sharing. The same filter is applied across\n",
    "  different parts of the image, making the network translational invariant. It\n",
    "  can recognize a feature anywhere in the input space.\n",
    "\n",
    "- **Spatial Hierarchies**: CNNs builds a hierarchy of features from lower to\n",
    "  higher levels of spatical abstraction, starting with simple features like\n",
    "  edges and textures, and moving on to more complex features.Early layers\n",
    "  detect simple patterns like edges and textures, while deeper layers identify\n",
    "  complex shapes and objects. This hierarchical representation ensures a deeper\n",
    "  understanding of image content. (This is present in DNN as well but CNNs do\n",
    "  it better for images)\n",
    "\n",
    "- **Scale and Rotational Invariance**: CNNs can be designed to be robust to\n",
    "  changes in scale and rotation. Pooling layers summarize neighboring neuron\n",
    "  outputs, imparting scale and rotational invariance to some extent, enhancing\n",
    "  feature detection across different variations.\n",
    "\n",
    "- **Reduced Computation**: Utilizing local connectivity and parameter sharing,\n",
    "  CNNs significantly reduce the number of trainable parameters compared to\n",
    "  fully connected DNNs. This computational efficiency makes training and\n",
    "  inference more manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cda9a8",
   "metadata": {},
   "source": [
    "\n",
    "MNIST CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e398f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================j\n",
    "#                             CNN CODE\n",
    "# =============================================================================j\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class CNN_MNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_MNIST, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(7 * 7 * 32, 256)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 10)  # 10 output classes for MNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = x.view(-1, 7 * 7 * 32)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=5):\n",
    "    # Create a SummaryWriter to log training information\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            \n",
    "            \n",
    "            if i % 100 == 99:\n",
    "                print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Loss: {running_loss / 100:.3f}\")\n",
    "                running_loss = 0.0\n",
    "                \n",
    "            \n",
    "\n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step()\n",
    "\n",
    "        accuracy = 100 * correct_predictions / total_samples\n",
    "        print(f\"Accuracy on epoch {epoch + 1}: {accuracy:.2f}%\")\n",
    "\n",
    "        # Log metrics to TensorBoard\n",
    "        writer.add_scalar('Loss/train', running_loss, epoch + 1)\n",
    "        writer.add_scalar('Accuracy/train', accuracy, epoch + 1)\n",
    "        writer.add_scalar('Learning_rate', optimizer.param_groups[0]['lr'], epoch + 1)\n",
    "\n",
    "        # Log histogram of model parameters (optional)\n",
    "        for name, param in model.named_parameters():\n",
    "            writer.add_histogram(name, param, epoch + 1)\n",
    "\n",
    "    print(\"Training finished!\")\n",
    "\n",
    "    # Close the SummaryWriter\n",
    "    writer.close()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_custom_model(model, dataset_name, transform=None, batch_size=64, fraction=1.0, num_epochs=5):\n",
    "    if not transform:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))  # Normalize inputs to range [-1, 1]\n",
    "        ])\n",
    "    # Create an instance of the DatasetLoader class\n",
    "    dataset_loader = DatasetLoader(dataset_name=dataset_name, transform=transform)\n",
    "\n",
    "    # Get the data loader for training set with specified fraction of the dataset and batch size\n",
    "    train_loader = dataset_loader.get_dataloader(batch_size=batch_size, shuffle=True, fraction=fraction)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Define learning rate scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    # Train the model\n",
    "    trained_model= train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=num_epochs)\n",
    "\n",
    "\n",
    "    # Capture the correct and incorrect examples after the training is done\n",
    "    correct_examples = []\n",
    "    incorrect_examples = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    trained_model.to(device)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = trained_model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_mask = predicted == labels\n",
    "            for image, true_label, pred_label, is_correct in zip(inputs, labels, predicted, correct_mask):\n",
    "                example_image = image.cpu().numpy()\n",
    "                true_label = true_label.item()\n",
    "                predicted_label = pred_label.item()\n",
    "                if is_correct:\n",
    "                    correct_examples.append((example_image, true_label, predicted_label))\n",
    "                else:\n",
    "                    incorrect_examples.append((example_image, true_label, predicted_label))\n",
    "    \n",
    "    return trained_model, correct_examples, incorrect_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f5acd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#                   Training the model\n",
    "# =============================================================================\n",
    "\n",
    "model,correct_examples, incorrect_examples = train_custom_model(CNN_MNIST(), \"MNIST\",batch_size=1024, fraction=0.3, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b80156",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "#                  Visualizing the results\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_examples(examples, title, example_count=6):\n",
    "    np.random.shuffle(examples)\n",
    "    num_examples = len(examples)\n",
    "    num_examples_to_show = min(num_examples, example_count)\n",
    "    rows = (num_examples_to_show + 2) // 3\n",
    "    cols = min(num_examples_to_show, 3)\n",
    "    plt.figure(figsize=(8, 3 * rows))\n",
    "\n",
    "    for i, (image, true_label, predicted_label) in enumerate(examples[:num_examples_to_show]):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        image = np.transpose(image, (1, 2, 0))  # Convert (C, H, W) to (H, W, C)\n",
    "        plt.imshow(image.squeeze(), cmap='gray')\n",
    "        plt.title(f'True: {true_label}\\nPredicted: {predicted_label}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.suptitle(title,fontsize=8)\n",
    "    plt.show()\n",
    "\n",
    "visualize_examples(correct_examples, f\"Correctly Classified Examples\")\n",
    "visualize_examples(incorrect_examples, \"Incorrectly Classified Examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32721be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "#                  Visualizing the trainnig\n",
    "# =============================================================================\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs\n",
    "\n",
    "# ============================================================================="
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
